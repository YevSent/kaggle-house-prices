{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction #\n",
    "\n",
    "Welcome to the feature engineering project for the [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition! This competition uses nearly the same data you used in the exercises of the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course. We'll collect together the work you did into a complete project which you can build off of with ideas of your own.\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "    <strong>Fork This Notebook!</strong><br>\n",
    "Create your own editable copy of this notebook by clicking on the <strong>Copy and Edit</strong> button in the top right corner.\n",
    "</blockquote>\n",
    "\n",
    "# Step 1 - Preliminaries #\n",
    "## Imports and Configuration ##\n",
    "\n",
    "We'll start by importing the packages we used in the exercises and setting some notebook defaults. Unhide this cell if you'd like to see the libraries we'll use:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_kg_hide-input": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:05.208301Z",
     "start_time": "2025-04-04T02:12:05.204917Z"
    }
   },
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from category_encoders import MEstimateEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Set Matplotlib defaults\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "# Mute warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path(\"../input/house-prices-advanced-regression-techniques/\") # @TODO change the directory for Kaggle.com, it uses \"../input/...\""
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing ##\n",
    "\n",
    "Before we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. The data we used in the course was a bit simpler than the competition data. For the *Ames* competition dataset, we'll need to:\n",
    "- **Load** the data from CSV files\n",
    "- **Clean** the data to fix any errors or inconsistencies\n",
    "- **Encode** the statistical data type (numeric, categorical)\n",
    "- **Impute** any missing values\n",
    "\n",
    "We'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:05.223659Z",
     "start_time": "2025-04-04T02:12:05.221562Z"
    }
   },
   "source": [
    "def load_data():\n",
    "    df_train = pd.read_csv(DATA_DIR / \"train.csv\", index_col=\"Id\")\n",
    "    df_test = pd.read_csv(DATA_DIR / \"test.csv\", index_col=\"Id\")\n",
    "    # Merge the splits so we can process them together\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    # Preprocessing\n",
    "    df = clean(df)\n",
    "    df = encode(df)\n",
    "    df = impute(df)\n",
    "    # Reform splits\n",
    "    df_train = df.loc[df_train.index, :]\n",
    "    df_test = df.loc[df_test.index, :]\n",
    "    return df_train, df_test\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data ###\n",
    "\n",
    "Some of the categorical features in this dataset have what are apparently typos in their categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these to `data_description.txt` shows us what needs cleaning. We'll take care of a couple of issues here, but you might want to evaluate this data further."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:05.228878Z",
     "start_time": "2025-04-04T02:12:05.226952Z"
    }
   },
   "source": [
    "def clean(df):\n",
    "    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\", \"Wd Shng\": \"Wd Sdng\", \"CmentBd\": \"CemntBd\"})\n",
    "    # Some values of GarageYrBlt are corrupt, so we'll replace them\n",
    "    # with the year the house was built\n",
    "    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n",
    "    # Names beginning with numbers are awkward to work with\n",
    "    df.rename(columns={\n",
    "        \"1stFlrSF\": \"FirstFlrSF\",\n",
    "        \"2ndFlrSF\": \"SecondFlrSF\",\n",
    "        \"3SsnPorch\": \"Threeseasonporch\",\n",
    "    }, inplace=True,\n",
    "    )\n",
    "    return df\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Statistical Data Type ###\n",
    "\n",
    "Pandas has Python types corresponding to the standard statistical types (numeric, categorical, etc.). Encoding each feature with its correct type helps ensure each feature is treated appropriately by whatever functions we use, and makes it easier for us to apply transformations consistently. This hidden cell defines the `encode` function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_kg_hide-input": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:05.236615Z",
     "start_time": "2025-04-04T02:12:05.233204Z"
    }
   },
   "source": [
    "\n",
    "# The numeric features are already encoded correctly (`float` for\n",
    "# continuous, `int` for discrete), but the categoricals we'll need to\n",
    "# do ourselves. Note in particular, that the `MSSubClass` feature is\n",
    "# read as an `int` type, but is actually a (nominative) categorical.\n",
    "\n",
    "# The nominative (unordered) categorical features\n",
    "features_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n",
    "\n",
    "\n",
    "# The ordinal (ordered) categorical features \n",
    "\n",
    "# Pandas calls the categories \"levels\"\n",
    "five_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\n",
    "ten_levels = list(range(10))\n",
    "\n",
    "ordered_levels = {\n",
    "    \"OverallQual\": ten_levels,\n",
    "    \"OverallCond\": ten_levels,\n",
    "    \"ExterQual\": five_levels,\n",
    "    \"ExterCond\": five_levels,\n",
    "    \"BsmtQual\": five_levels,\n",
    "    \"BsmtCond\": five_levels,\n",
    "    \"HeatingQC\": five_levels,\n",
    "    \"KitchenQual\": five_levels,\n",
    "    \"FireplaceQu\": five_levels,\n",
    "    \"GarageQual\": five_levels,\n",
    "    \"GarageCond\": five_levels,\n",
    "    \"PoolQC\": five_levels,\n",
    "    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n",
    "    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n",
    "    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n",
    "    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n",
    "    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n",
    "    \"CentralAir\": [\"N\", \"Y\"],\n",
    "    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n",
    "    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n",
    "}\n",
    "\n",
    "# Add a None level for missing values\n",
    "ordered_levels = {key: [\"None\"] + value for key, value in\n",
    "                  ordered_levels.items()}\n",
    "\n",
    "\n",
    "def encode(df):\n",
    "    # Nominal categories\n",
    "    for name in features_nom:\n",
    "        df[name] = df[name].astype(\"category\")\n",
    "        # Add a None category for missing values\n",
    "        if \"None\" not in df[name].cat.categories:\n",
    "            df[name] = df[name].cat.add_categories(\"None\")\n",
    "    # Ordinal categories\n",
    "    for name, levels in ordered_levels.items():\n",
    "        df[name] = df[name].astype(CategoricalDtype(levels,\n",
    "                                                    ordered=True))\n",
    "    return df\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values ###\n",
    "\n",
    "Handling missing values now will make the feature engineering go more smoothly. We'll impute `0` for missing numeric values and `\"None\"` for missing categorical values. You might like to experiment with other imputation strategies. In particular, you could try creating \"missing value\" indicators: `1` whenever a value was imputed and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:05.242346Z",
     "start_time": "2025-04-04T02:12:05.240547Z"
    }
   },
   "source": [
    "def impute(df):\n",
    "    for name in df.select_dtypes(\"number\"):\n",
    "        df[name] = df[name].fillna(0)\n",
    "    for name in df.select_dtypes(\"category\"):\n",
    "        df[name] = df[name].fillna(\"None\")\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data ##\n",
    "\n",
    "And now we can call the data loader and get the processed data splits:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:05.276119Z",
     "start_time": "2025-04-04T02:12:05.246946Z"
    }
   },
   "source": [
    "df_train, df_test = load_data()"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Baseline ##\n",
    "\n",
    "Finally, let's establish a baseline score to judge our feature engineering against.\n",
    "\n",
    "Here is the function we created in Lesson 1 that will compute the cross-validated RMSLE score for a feature set. We've used XGBoost for our model, but you might want to experiment with other models.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_kg_hide-input": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:05.286404Z",
     "start_time": "2025-04-04T02:12:05.284570Z"
    }
   },
   "source": [
    "\n",
    "def score_dataset(X, y, model=XGBRegressor()):\n",
    "    # Label encoding for categoricals\n",
    "    #\n",
    "    # Label encoding is good for XGBoost and RandomForest, but one-hot\n",
    "    # would be better for models like Lasso or Ridge. The `cat.codes`\n",
    "    # attribute holds the category levels.\n",
    "    for colname in X.select_dtypes([\"category\"]):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n",
    "    log_y = np.log(y)\n",
    "    score = cross_val_score(\n",
    "        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n",
    "    )\n",
    "    score = -1 * score.mean()\n",
    "    score = np.sqrt(score)\n",
    "    return score\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reuse this scoring function anytime we want to try out a new feature set. We'll run it now on the processed data with no additional features and get a baseline score:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:06.347991Z",
     "start_time": "2025-04-04T02:12:05.293305Z"
    }
   },
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "baseline_score = score_dataset(X, y)\n",
    "print(f\"Baseline score: {baseline_score:.5f} RMSLE\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 0.14374 RMSLE\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline score helps us to know whether some set of features we've assembled has actually led to any improvement or not.\n",
    "\n",
    "# Step 2 - Feature Utility Scores #\n",
    "\n",
    "In Lesson 2 we saw how to use mutual information to compute a *utility score* for a feature, giving you an indication of how much potential the feature has. This hidden cell defines the two utility functions we used, `make_mi_scores` and `plot_mi_scores`: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_kg_hide-input": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:06.364368Z",
     "start_time": "2025-04-04T02:12:06.362267Z"
    }
   },
   "source": [
    "\n",
    "def make_mi_scores(X, y):\n",
    "    X = X.copy()\n",
    "    for colname in X.select_dtypes([\"object\", \"category\"]):\n",
    "        X[colname], _ = X[colname].factorize()\n",
    "    # All discrete features should now have integer dtypes\n",
    "    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our feature scores again:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:07.449673Z",
     "start_time": "2025-04-04T02:12:06.374616Z"
    }
   },
   "source": [
    "X = df_train.copy()\n",
    "y = X.pop(\"SalePrice\")\n",
    "\n",
    "mi_scores = make_mi_scores(X, y)\n",
    "mi_scores"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual     0.571457\n",
       "Neighborhood    0.526220\n",
       "GrLivArea       0.430395\n",
       "YearBuilt       0.407974\n",
       "LotArea         0.394468\n",
       "                  ...   \n",
       "PoolQC          0.000000\n",
       "MiscFeature     0.000000\n",
       "MiscVal         0.000000\n",
       "MoSold          0.000000\n",
       "YrSold          0.000000\n",
       "Name: MI Scores, Length: 79, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have a number of features that are highly informative and also some that don't seem to be informative at all (at least by themselves). As we talked about in Tutorial 2, the top scoring features will usually pay-off the most during feature development, so it could be a good idea to focus your efforts on those. On the other hand, training on uninformative features can lead to overfitting. So, the features with 0.0 scores we'll drop entirely:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:07.461726Z",
     "start_time": "2025-04-04T02:12:07.459942Z"
    }
   },
   "source": [
    "def drop_uninformative(df, mi_scores):\n",
    "    return df.loc[:, mi_scores > 0.0]"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Create Features #"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:07.476796Z",
     "start_time": "2025-04-04T02:12:07.475388Z"
    }
   },
   "source": [
    "def label_encode(df):\n",
    "    X = df.copy()\n",
    "    for colname in X.select_dtypes([\"category\"]):\n",
    "        X[colname] = X[colname].cat.codes\n",
    "    return X"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A label encoding is okay for any kind of categorical feature when you're using a tree-ensemble like XGBoost, even for unordered categories. If you wanted to try a linear regression model (also popular in this competition), you would instead want to use a one-hot encoding, especially for the features with unordered categories.\n",
    "\n",
    "## Create Features with Pandas ##\n",
    "\n",
    "This cell reproduces the work you did in Exercise 3, where you applied strategies for creating features in Pandas. Modify or add to these functions to try out other feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_kg_hide-input": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:07.481239Z",
     "start_time": "2025-04-04T02:12:07.478982Z"
    }
   },
   "source": [
    "def interactions(df):\n",
    "    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n",
    "    X = X.mul(df.GrLivArea, axis=0)\n",
    "    #X = X.join(pd.get_dummies(df.BsmtQual, prefix=\"Bsmt\"))\n",
    "    #X = X.mul(df.TotalBsmtSF, axis=0) # RMSE increases\n",
    "    return X\n",
    "\n",
    "\n",
    "def counts(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"PorchTypes\"] = df[[\n",
    "        \"WoodDeckSF\",\n",
    "        \"OpenPorchSF\",\n",
    "        \"EnclosedPorch\",\n",
    "        \"Threeseasonporch\",\n",
    "        \"ScreenPorch\",\n",
    "    ]].gt(0.0).sum(axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def group_transforms(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n",
    "    #X[\"MedNhbdAreaStd\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"std\") # RMSE increases\n",
    "    return X\n",
    "\n",
    "# combine different quality features\n",
    "def quality_features(df):\n",
    "    X = pd.DataFrame()\n",
    "    X[\"QualNhbd\"] = df.groupby(\"Neighborhood\")[\"OverallQual\"].transform(\"mean\")\n",
    "    #X[\"QualNhbdStd\"] = df.groupby(\"Neighborhood\")[\"OverallQual\"].transform(\"std\") # RMSE increases\n",
    "    #X[\"CondNhbd\"] = df.groupby(\"Neighborhood\")[\"OverallCond\"].transform(\"mean\") # RMSE increases\n",
    "    #X[\"ProductQualCond\"] = df[\"OverallQual\"] * df[\"OverallCond\"] # RMSE increases\n",
    "    return X"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some ideas for other transforms you could explore:\n",
    "- Interactions between the quality `Qual` and condition `Cond` features. `OverallQual`, for instance, was a high-scoring feature. You could try combining it with `OverallCond` by converting both to integer type and taking a product.\n",
    "- Square roots of area features. This would convert units of square feet to just feet.\n",
    "- Logarithms of numeric features. If a feature has a skewed distribution, applying a logarithm can help normalize it.\n",
    "- Interactions between numeric and categorical features that describe the same thing. You could look at interactions between `BsmtQual` and `TotalBsmtSF`, for instance.\n",
    "- Other group statistics in `Neighboorhood`. We did the median of `GrLivArea`. Looking at `mean`, `std`, or `count` could be interesting. You could also try combining the group statistics with other features. Maybe the *difference* of `GrLivArea` and the median is important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding ##\n",
    "\n",
    "Needing a separate holdout set to create a target encoding is rather wasteful of data. In *Tutorial 6* we used 25% of our dataset just to encode a single feature, `Zipcode`. The data from the other features in that 25% we didn't get to use at all.\n",
    "\n",
    "There is, however, a way you can use target encoding without having to use held-out encoding data. It's basically the same trick used in cross-validation:\n",
    "1. Split the data into folds, each fold having two splits of the dataset.\n",
    "2. Train the encoder on one split but transform the values of the other.\n",
    "3. Repeat for all the splits.\n",
    "\n",
    "This way, training and transformation always take place on independent sets of data, just like when you use a holdout set but without any data going to waste.\n",
    "\n",
    "In the next hidden cell is a wrapper you can use with any target encoder:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "_kg_hide-input": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:07.576481Z",
     "start_time": "2025-04-04T02:12:07.573897Z"
    }
   },
   "source": [
    "\n",
    "class CrossFoldEncoder:\n",
    "    def __init__(self, encoder, **kwargs):\n",
    "        self.encoder_ = encoder\n",
    "        self.kwargs_ = kwargs  # keyword arguments for the encoder\n",
    "        self.cv_ = KFold(n_splits=5)\n",
    "\n",
    "    # Fit an encoder on one split and transform the feature on the\n",
    "    # other. Iterating over the splits in all folds gives a complete\n",
    "    # transformation. We also now have one trained encoder on each\n",
    "    # fold.\n",
    "    def fit_transform(self, X, y, cols):\n",
    "        self.fitted_encoders_ = []\n",
    "        self.cols_ = cols\n",
    "        X_encoded = []\n",
    "        for idx_encode, idx_train in self.cv_.split(X):\n",
    "            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n",
    "            fitted_encoder.fit(\n",
    "                X.iloc[idx_encode, :], y.iloc[idx_encode],\n",
    "            )\n",
    "            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n",
    "            self.fitted_encoders_.append(fitted_encoder)\n",
    "        X_encoded = pd.concat(X_encoded)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded\n",
    "\n",
    "    # To transform the test data, average the encodings learned from\n",
    "    # each fold.\n",
    "    def transform(self, X):\n",
    "        from functools import reduce\n",
    "\n",
    "        X_encoded_list = []\n",
    "        for fitted_encoder in self.fitted_encoders_:\n",
    "            X_encoded = fitted_encoder.transform(X)\n",
    "            X_encoded_list.append(X_encoded[self.cols_])\n",
    "        X_encoded = reduce(\n",
    "            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n",
    "        ) / len(X_encoded_list)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it like:\n",
    "\n",
    "```\n",
    "encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n",
    "X_encoded = encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n",
    "```\n",
    "\n",
    "You can turn any of the encoders from the [`category_encoders`](http://contrib.scikit-learn.org/category_encoders/) library into a cross-fold encoder. The [`CatBoostEncoder`](http://contrib.scikit-learn.org/category_encoders/catboost.html) would be worth trying. It's similar to `MEstimateEncoder` but uses some tricks to better prevent overfitting. Its smoothing parameter is called `a` instead of `m`.\n",
    "\n",
    "## Create Final Feature Set ##\n",
    "\n",
    "Now let's combine everything together. Putting the transformations into separate functions makes it easier to experiment with various combinations. The ones I left uncommented I found gave the best results. You should experiment with you own ideas though! Modify any of these transformations or come up with some of your own to add to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:09.890353Z",
     "start_time": "2025-04-04T02:12:07.695760Z"
    }
   },
   "source": [
    "def create_features(df, df_test=None):\n",
    "    X = df.copy()\n",
    "    y = X.pop(\"SalePrice\")\n",
    "    mi_scores = make_mi_scores(X, y)\n",
    "\n",
    "    # Combine splits if test data is given\n",
    "    #\n",
    "    # If we're creating features for test set predictions, we should\n",
    "    # use all the data we have available. After creating our features,\n",
    "    # we'll recreate the splits.\n",
    "    if df_test is not None:\n",
    "        X_test = df_test.copy()\n",
    "        X_test.pop(\"SalePrice\")\n",
    "        X = pd.concat([X, X_test])\n",
    "\n",
    "    X = drop_uninformative(X, mi_scores)\n",
    "\n",
    "    X = X.join(interactions(X))\n",
    "    \n",
    "    X = X.join(counts(X))\n",
    "    \n",
    "    X = X.join(group_transforms(X))\n",
    "\n",
    "    X = label_encode(X)\n",
    "\n",
    "    X = X.join(quality_features(X))\n",
    "\n",
    "    # Reform splits\n",
    "    if df_test is not None:\n",
    "        X_test = X.loc[df_test.index, :]\n",
    "        X.drop(df_test.index, inplace=True)\n",
    "\n",
    "    # Lesson 6 - Target Encoder\n",
    "    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n",
    "    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n",
    "    \n",
    "    if df_test is not None:\n",
    "        X_test = X_test.join(encoder.transform(X_test))\n",
    "\n",
    "    if df_test is not None:\n",
    "        return X, X_test\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n",
    "df_train, df_test = load_data()\n",
    "X_train = create_features(df_train)\n",
    "y_train = df_train.loc[:, \"SalePrice\"]\n",
    "\n",
    "score_dataset(X_train, y_train)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.13453051336747965)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Hyperparameter Tuning #\n",
    "\n",
    "At this stage, you might like to do some hyperparameter tuning with XGBoost before creating your final submission."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:12:09.918131Z",
     "start_time": "2025-04-04T02:12:09.892562Z"
    }
   },
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    xgb_params = dict(\n",
    "        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
    "        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n",
    "        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n",
    "        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n",
    "    )\n",
    "    xgb = XGBRegressor(**xgb_params)\n",
    "    return score_dataset(X_train, y_train, xgb)\n",
    "\n",
    "#study = optuna.create_study(direction=\"minimize\")\n",
    "#study.optimize(objective, n_trials=5)\n",
    "#xgb_params = study.best_params\n",
    "#xgb_params['num_parallel_tree'] = 1"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:56:57.842234Z",
     "start_time": "2025-04-04T02:56:35.473639Z"
    }
   },
   "source": [
    "X_train = create_features(df_train)\n",
    "y_train = df_train.loc[:, \"SalePrice\"]\n",
    "\n",
    "xgb_params = dict(\n",
    "    max_depth=4,           # maximum depth of each tree - try 2 to 10\n",
    "    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n",
    "    n_estimators=3000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n",
    "    min_child_weight=5,    # minimum number of houses in a leaf - try 1 to 10\n",
    "    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n",
    "    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n",
    "    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n",
    "    reg_lambda=2.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n",
    "    num_parallel_tree=1,   # set > 1 for boosted random forests\n",
    ")\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "prev_score = [0.11756136479624747]\n",
    "cur_score = score_dataset(X_train, y_train, xgb)\n",
    "print(cur_score <= prev_score)\n",
    "cur_score"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.12048649911163768)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:13:12.960196Z",
     "start_time": "2025-04-04T02:13:01.493652Z"
    }
   },
   "source": [
    "X_train, X_test = create_features(df_train, df_test)\n",
    "y_train = df_train.loc[:, \"SalePrice\"]\n",
    "\n",
    "xgb = XGBRegressor(**xgb_params)\n",
    "# XGB minimizes MSE, but competition loss is RMSLE\n",
    "# So, we need to log-transform y to train and exp-transform the predictions\n",
    "xgb.fit(X_train, np.log(y))\n",
    "predictions = np.exp(xgb.predict(X_test))\n",
    "\n",
    "output = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n",
    "output.to_csv('submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Grid Search for Cross-Validation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T02:24:17.510099Z",
     "start_time": "2025-04-04T02:13:12.973508Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/data.py:407\u001B[39m, in \u001B[36mpandas_feature_info\u001B[39m\u001B[34m(data, meta, feature_names, feature_types, enable_categorical)\u001B[39m\n\u001B[32m    406\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m407\u001B[39m     new_feature_types.append(\u001B[43m_pandas_dtype_mapper\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[32m    408\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "\u001B[31mKeyError\u001B[39m: 'category'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:866\u001B[39m, in \u001B[36m_fit_and_score\u001B[39m\u001B[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[39m\n\u001B[32m    865\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m866\u001B[39m         \u001B[43mestimator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1222\u001B[39m, in \u001B[36mXGBModel.fit\u001B[39m\u001B[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001B[39m\n\u001B[32m   1221\u001B[39m evals_result: TrainingCallback.EvalsLog = {}\n\u001B[32m-> \u001B[39m\u001B[32m1222\u001B[39m train_dmatrix, evals = \u001B[43m_wrap_evaluation_matrices\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1223\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1224\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m=\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1225\u001B[39m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m=\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1226\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1227\u001B[39m \u001B[43m    \u001B[49m\u001B[43mqid\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1228\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1229\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_margin\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_margin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1230\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1231\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_set\u001B[49m\u001B[43m=\u001B[49m\u001B[43meval_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1232\u001B[39m \u001B[43m    \u001B[49m\u001B[43msample_weight_eval_set\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight_eval_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1233\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_margin_eval_set\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_margin_eval_set\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1234\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_group\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1235\u001B[39m \u001B[43m    \u001B[49m\u001B[43meval_qid\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1236\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_dmatrix\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_dmatrix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1237\u001B[39m \u001B[43m    \u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1238\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1239\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1241\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m.objective):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:628\u001B[39m, in \u001B[36m_wrap_evaluation_matrices\u001B[39m\u001B[34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001B[39m\n\u001B[32m    626\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001B[39;00m\n\u001B[32m    627\u001B[39m \u001B[33;03mway.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m628\u001B[39m train_dmatrix = \u001B[43mcreate_dmatrix\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    629\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    630\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m=\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    631\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[43m    \u001B[49m\u001B[43mqid\u001B[49m\u001B[43m=\u001B[49m\u001B[43mqid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    633\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m=\u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_margin\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_margin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    635\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    636\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    637\u001B[39m \u001B[43m    \u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m=\u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    638\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    639\u001B[39m \u001B[43m    \u001B[49m\u001B[43mref\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    640\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    642\u001B[39m n_validation = \u001B[32m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m eval_set \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(eval_set)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1137\u001B[39m, in \u001B[36mXGBModel._create_dmatrix\u001B[39m\u001B[34m(self, ref, **kwargs)\u001B[39m\n\u001B[32m   1136\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mQuantileDMatrix\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1138\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mref\u001B[49m\u001B[43m=\u001B[49m\u001B[43mref\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnthread\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_bin\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_bin\u001B[49m\n\u001B[32m   1139\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1140\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:  \u001B[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:1614\u001B[39m, in \u001B[36mQuantileDMatrix.__init__\u001B[39m\u001B[34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, max_quantile_batches, data_split_mode)\u001B[39m\n\u001B[32m   1609\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1610\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mIf data iterator is used as input, data like label should be \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1611\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mspecified as batch argument.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1612\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1614\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_init\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1615\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1616\u001B[39m \u001B[43m    \u001B[49m\u001B[43mref\u001B[49m\u001B[43m=\u001B[49m\u001B[43mref\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1617\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1618\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1619\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbase_margin\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbase_margin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1620\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1621\u001B[39m \u001B[43m    \u001B[49m\u001B[43mqid\u001B[49m\u001B[43m=\u001B[49m\u001B[43mqid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1622\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabel_lower_bound\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabel_lower_bound\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1623\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabel_upper_bound\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlabel_upper_bound\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1624\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_weights\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1625\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1626\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1627\u001B[39m \u001B[43m    \u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m=\u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1628\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_quantile_blocks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_quantile_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1629\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:1678\u001B[39m, in \u001B[36mQuantileDMatrix._init\u001B[39m\u001B[34m(self, data, ref, enable_categorical, max_quantile_blocks, **meta)\u001B[39m\n\u001B[32m   1669\u001B[39m ret = _LIB.XGQuantileDMatrixCreateFromCallback(\n\u001B[32m   1670\u001B[39m     \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1671\u001B[39m     it.proxy.handle,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1676\u001B[39m     ctypes.byref(handle),\n\u001B[32m   1677\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1678\u001B[39m \u001B[43mit\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1679\u001B[39m \u001B[38;5;66;03m# delay check_call to throw intermediate exception first\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:572\u001B[39m, in \u001B[36mDataIter.reraise\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    571\u001B[39m \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m572\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:553\u001B[39m, in \u001B[36mDataIter._handle_exception\u001B[39m\u001B[34m(self, fn, dft_ret)\u001B[39m\n\u001B[32m    552\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m553\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    554\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[32m    555\u001B[39m     \u001B[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001B[39;00m\n\u001B[32m    556\u001B[39m     \u001B[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001B[39;00m\n\u001B[32m    557\u001B[39m     \u001B[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:640\u001B[39m, in \u001B[36mDataIter._next_wrapper.<locals>.<lambda>\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    639\u001B[39m \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m640\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._handle_exception(\u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnext\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m)\u001B[49m), \u001B[32m0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/data.py:1654\u001B[39m, in \u001B[36mSingleBatchInternalIter.next\u001B[39m\u001B[34m(self, input_data)\u001B[39m\n\u001B[32m   1653\u001B[39m \u001B[38;5;28mself\u001B[39m.it += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1654\u001B[39m \u001B[43minput_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1655\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:729\u001B[39m, in \u001B[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    728\u001B[39m     kwargs[k] = arg\n\u001B[32m--> \u001B[39m\u001B[32m729\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/core.py:620\u001B[39m, in \u001B[36mDataIter._next_wrapper.<locals>.input_data\u001B[39m\u001B[34m(data, feature_names, feature_types, **kwargs)\u001B[39m\n\u001B[32m    619\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m     new, cat_codes, feature_names, feature_types = \u001B[43m_proxy_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    621\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    623\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    624\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_enable_categorical\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    625\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/data.py:1707\u001B[39m, in \u001B[36m_proxy_transform\u001B[39m\u001B[34m(data, feature_names, feature_types, enable_categorical)\u001B[39m\n\u001B[32m   1706\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _is_pandas_df(data):\n\u001B[32m-> \u001B[39m\u001B[32m1707\u001B[39m     df, feature_names, feature_types = \u001B[43m_transform_pandas_df\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1708\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_types\u001B[49m\n\u001B[32m   1709\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1710\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m df, \u001B[38;5;28;01mNone\u001B[39;00m, feature_names, feature_types\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/data.py:640\u001B[39m, in \u001B[36m_transform_pandas_df\u001B[39m\u001B[34m(data, enable_categorical, feature_names, feature_types, meta)\u001B[39m\n\u001B[32m    638\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDataFrame for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmeta\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m cannot have multiple columns\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m640\u001B[39m feature_names, feature_types = \u001B[43mpandas_feature_info\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    641\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable_categorical\u001B[49m\n\u001B[32m    642\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    644\u001B[39m arrays = pandas_transform_data(data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/data.py:409\u001B[39m, in \u001B[36mpandas_feature_info\u001B[39m\u001B[34m(data, meta, feature_names, feature_types, enable_categorical)\u001B[39m\n\u001B[32m    408\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m409\u001B[39m             \u001B[43m_invalid_dataframe_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    411\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m feature_types \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m meta \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/xgboost/data.py:372\u001B[39m, in \u001B[36m_invalid_dataframe_dtype\u001B[39m\u001B[34m(data)\u001B[39m\n\u001B[32m    371\u001B[39m msg = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtype_err\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_ENABLE_CAT_ERR\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merr\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m372\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[31mValueError\u001B[39m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:MSSubClass: category, MSZoning: category, Street: category, Alley: category, LotShape: category, LandContour: category, Utilities: category, LotConfig: category, LandSlope: category, Neighborhood: category, Condition1: category, Condition2: category, BldgType: category, HouseStyle: category, OverallQual: category, OverallCond: category, RoofStyle: category, RoofMatl: category, Exterior1st: category, Exterior2nd: category, MasVnrType: category, ExterQual: category, ExterCond: category, Foundation: category, BsmtQual: category, BsmtCond: category, BsmtExposure: category, BsmtFinType1: category, BsmtFinType2: category, Heating: category, HeatingQC: category, CentralAir: category, Electrical: category, KitchenQual: category, Functional: category, FireplaceQu: category, GarageType: category, GarageFinish: category, GarageQual: category, GarageCond: category, PavedDrive: category, PoolQC: category, Fence: category, MiscFeature: category, SaleType: category, SaleCondition: category",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m      2\u001B[39m param_grid = \u001B[38;5;28mdict\u001B[39m(\n\u001B[32m      3\u001B[39m     max_depth=[\u001B[32m2\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m, \u001B[32m6\u001B[39m, \u001B[32m7\u001B[39m, \u001B[32m8\u001B[39m, \u001B[32m9\u001B[39m, \u001B[32m10\u001B[39m],           \u001B[38;5;66;03m# maximum depth of each tree - try 2 to 10\u001B[39;00m\n\u001B[32m      4\u001B[39m     learning_rate=[\u001B[32m0.0001\u001B[39m, \u001B[32m0.001\u001B[39m, \u001B[32m0.01\u001B[39m, \u001B[32m0.1\u001B[39m],    \u001B[38;5;66;03m# effect of each tree - try 0.0001 to 0.1\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     11\u001B[39m     num_parallel_tree=[\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m]\n\u001B[32m     12\u001B[39m )\n\u001B[32m     14\u001B[39m grid_search = GridSearchCV(xgb, param_grid, cv=\u001B[32m5\u001B[39m, scoring=\u001B[33m'\u001B[39m\u001B[33maccuracy\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mBest parameters found:\u001B[39m\u001B[33m\"\u001B[39m, grid_search.best_params_)\n\u001B[32m     18\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mBest cross-validation score:\u001B[39m\u001B[33m\"\u001B[39m, grid_search.best_score_)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001B[39m, in \u001B[36mBaseSearchCV.fit\u001B[39m\u001B[34m(self, X, y, **params)\u001B[39m\n\u001B[32m   1018\u001B[39m     results = \u001B[38;5;28mself\u001B[39m._format_results(\n\u001B[32m   1019\u001B[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[32m   1020\u001B[39m     )\n\u001B[32m   1022\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[32m-> \u001B[39m\u001B[32m1024\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1026\u001B[39m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[32m   1027\u001B[39m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[32m   1028\u001B[39m first_test_score = all_out[\u001B[32m0\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33mtest_scores\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001B[39m, in \u001B[36mGridSearchCV._run_search\u001B[39m\u001B[34m(self, evaluate_candidates)\u001B[39m\n\u001B[32m   1569\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[32m   1570\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1571\u001B[39m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001B[39m, in \u001B[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[39m\u001B[34m(candidate_params, cv, more_results)\u001B[39m\n\u001B[32m    962\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.verbose > \u001B[32m0\u001B[39m:\n\u001B[32m    963\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[32m    964\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[33m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[33m candidates,\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    965\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m fits\u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m    966\u001B[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001B[32m    967\u001B[39m         )\n\u001B[32m    968\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m970\u001B[39m out = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    974\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    978\u001B[39m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    979\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    982\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    983\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    984\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    985\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    986\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    988\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) < \u001B[32m1\u001B[39m:\n\u001B[32m    989\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    990\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mNo fits were performed. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    991\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWas the CV iterator empty? \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    992\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mWere there no candidates?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    993\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     72\u001B[39m config = get_config()\n\u001B[32m     73\u001B[39m iterable_with_config = (\n\u001B[32m     74\u001B[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     76\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1916\u001B[39m     output = \u001B[38;5;28mself\u001B[39m._get_sequential_output(iterable)\n\u001B[32m   1917\u001B[39m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m1918\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1920\u001B[39m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[32m   1921\u001B[39m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[32m   1922\u001B[39m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[32m   1923\u001B[39m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[32m   1924\u001B[39m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[32m   1925\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._lock:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001B[39m, in \u001B[36mParallel._get_sequential_output\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   1845\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_batches += \u001B[32m1\u001B[39m\n\u001B[32m   1846\u001B[39m \u001B[38;5;28mself\u001B[39m.n_dispatched_tasks += \u001B[32m1\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1847\u001B[39m res = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1848\u001B[39m \u001B[38;5;28mself\u001B[39m.n_completed_tasks += \u001B[32m1\u001B[39m\n\u001B[32m   1849\u001B[39m \u001B[38;5;28mself\u001B[39m.print_progress()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:139\u001B[39m, in \u001B[36m_FuncWrapper.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    137\u001B[39m     config = {}\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(**config):\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/dev/kaggle-house-prices/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:883\u001B[39m, in \u001B[36m_fit_and_score\u001B[39m\u001B[34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[39m\n\u001B[32m    881\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m return_train_score:\n\u001B[32m    882\u001B[39m                 train_scores = error_score\n\u001B[32m--> \u001B[39m\u001B[32m883\u001B[39m     result[\u001B[33m\"\u001B[39m\u001B[33mfit_error\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[43mformat_exc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    884\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    885\u001B[39m     result[\u001B[33m\"\u001B[39m\u001B[33mfit_error\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/traceback.py:184\u001B[39m, in \u001B[36mformat_exc\u001B[39m\u001B[34m(limit, chain)\u001B[39m\n\u001B[32m    182\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mformat_exc\u001B[39m(limit=\u001B[38;5;28;01mNone\u001B[39;00m, chain=\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[32m    183\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Like print_exc() but return a string.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m184\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[43mformat_exception\u001B[49m\u001B[43m(\u001B[49m\u001B[43msys\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexception\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlimit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchain\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/traceback.py:140\u001B[39m, in \u001B[36mformat_exception\u001B[39m\u001B[34m(exc, value, tb, limit, chain)\u001B[39m\n\u001B[32m    138\u001B[39m value, tb = _parse_value_tb(exc, value, tb)\n\u001B[32m    139\u001B[39m te = TracebackException(\u001B[38;5;28mtype\u001B[39m(value), value, tb, limit=limit, compact=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m140\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mte\u001B[49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/traceback.py:986\u001B[39m, in \u001B[36mTracebackException.format\u001B[39m\u001B[34m(self, chain, _ctx)\u001B[39m\n\u001B[32m    984\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m exc.stack:\n\u001B[32m    985\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m _ctx.emit(\u001B[33m'\u001B[39m\u001B[33mTraceback (most recent call last):\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m986\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m _ctx.emit(\u001B[43mexc\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstack\u001B[49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    987\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m _ctx.emit(exc.format_exception_only())\n\u001B[32m    988\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m _ctx.exception_group_depth > \u001B[38;5;28mself\u001B[39m.max_group_depth:\n\u001B[32m    989\u001B[39m     \u001B[38;5;66;03m# exception group, but depth exceeds limit\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/traceback.py:543\u001B[39m, in \u001B[36mStackSummary.format\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    541\u001B[39m count = \u001B[32m0\u001B[39m\n\u001B[32m    542\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m frame_summary \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m543\u001B[39m     formatted_frame = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat_frame_summary\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe_summary\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    544\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m formatted_frame \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    545\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/traceback.py:467\u001B[39m, in \u001B[36mStackSummary.format_frame_summary\u001B[39m\u001B[34m(self, frame_summary)\u001B[39m\n\u001B[32m    461\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Format the lines for a single FrameSummary.\u001B[39;00m\n\u001B[32m    462\u001B[39m \n\u001B[32m    463\u001B[39m \u001B[33;03mReturns a string representing one frame involved in the stack. This\u001B[39;00m\n\u001B[32m    464\u001B[39m \u001B[33;03mgets called for every frame to be printed in the stack summary.\u001B[39;00m\n\u001B[32m    465\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    466\u001B[39m row = []\n\u001B[32m--> \u001B[39m\u001B[32m467\u001B[39m row.append(\u001B[33;43m'\u001B[39;49m\u001B[33;43m  File \u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m, line \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[33;43m, in \u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[33;43m'\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    468\u001B[39m \u001B[43m    \u001B[49m\u001B[43mframe_summary\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_summary\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlineno\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe_summary\u001B[49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m    469\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m frame_summary.line:\n\u001B[32m    470\u001B[39m     stripped_line = frame_summary.line.strip()\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 28,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = dict(\n",
    "    max_depth=[5, 6, 7, 8],           # maximum depth of each tree - try 2 to 10\n",
    "    learning_rate=[0.001, 0.01, 0.1],    # effect of each tree - try 0.0001 to 0.1\n",
    "    n_estimators=[4000, 5000, 6000],     # number of trees (that is, boosting rounds) - try 1000 to 8000\n",
    "    min_child_weight=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],    # minimum number of houses in a leaf - try 1 to 10\n",
    "    colsample_bytree=[0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1],  # fraction of features (columns) per tree - try 0.2 to 1.0\n",
    "    subsample=[0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1],         # fraction of instances (rows) per tree - try 0.2 to 1.0\n",
    "    reg_alpha=[0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], # L1 regularization (like LASSO) - try 0.0 to 10.0\n",
    "    reg_lambda=[0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], # L2 regularization (like Ridge) - try 0.0 to 10.0\n",
    "    num_parallel_tree=[1, 2, 3, 4, 5]\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = list(zip(xgb.feature_importances_, X_train.columns))\n",
    "sorted_feature_importances = sorted(feature_importances, key=lambda x: x[0], reverse=True)\n",
    "for score, name in sorted_feature_importances:\n",
    "    print(round(score, 2), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shap\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_train, y_train, random_state=1)\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "regressor.fit(train_X, train_y)\n",
    "\n",
    "explainer = shap.TreeExplainer(regressor)\n",
    "\n",
    "shap_values = explainer.shap_values(val_X)\n",
    "shap.summary_plot(shap_values, val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_to_show = 10\n",
    "data_for_prediction = val_X.iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n",
    "data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n",
    "print(regressor.predict(data_for_prediction_array))\n",
    "\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values, data_for_prediction)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
